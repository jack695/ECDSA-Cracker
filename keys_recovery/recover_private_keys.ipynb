{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utxo_utils.crypto.signature import verifySignature\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import ecdsa\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from sympy import Matrix\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# DATA HANDLING #\n",
    "#################\n",
    "def is_row_valid(row):\n",
    "    try:\n",
    "        verifySignature(row[\"pubkey\"], f\"{int(row[\"r\"], 16):064x}\" + f\"{int(row[\"s\"], 16):064x}\", row[\"message digest\"])\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def collect_file_paths(folders: list[str]):\n",
    "    signatures_files = []\n",
    "    for folder in folders:\n",
    "        signatures_files += glob.glob(os.path.join(folder, \"*.parquet\"))\n",
    "    return signatures_files\n",
    "\n",
    "\n",
    "def read_raw_data(signatures_folders: list[str], check_signatures: bool = False):\n",
    "    signatures_files = collect_file_paths(signatures_folders)\n",
    "    df = pd.concat(\n",
    "        pd.read_parquet(parquet_file)\n",
    "        for parquet_file in signatures_files\n",
    "    )\n",
    "\n",
    "    # Last filtering step: remove possible unvalid signatures\n",
    "    filtered_df = df if not check_signatures else df[df.apply(is_row_valid, axis=1)]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\" Read the signature files and group the rows by pubkey and r such that it contains two message digests and two s.\"\"\"\n",
    "    # Group by pubkey and r. Keep two records for every row.\n",
    "    grouped_df = (\n",
    "        df.sort_values(by=[\"block_timestamp\"])\n",
    "        .groupby(by=[\"pubkey\", \"r\", \"s\", \"message digest\"], sort=False)\n",
    "        .head(1)\n",
    "        .groupby(by=[\"pubkey\", \"r\"], sort=False)\n",
    "        .aggregate(\n",
    "            s_cnt=(\"s\", \"nunique\"),\n",
    "            digest_cnt=(\"message digest\", \"nunique\"),\n",
    "            s=(\"s\", lambda s: s.drop_duplicates().head(2)),\n",
    "            digests=(\"message digest\", lambda s: s.drop_duplicates().head(2)),\n",
    "            chain=(\"chain\", lambda s: s.drop_duplicates()),\n",
    "            block_timestamps=(\"block_timestamp\", lambda s: s.head(2)),\n",
    "        )\n",
    "    )\n",
    "    # Keep the records in presence of a repeated nonce\n",
    "    grouped_df = grouped_df[(grouped_df[\"s_cnt\"] > 1) & (grouped_df[\"digest_cnt\"] > 1)].drop(columns=[\"s_cnt\", \"digest_cnt\"])\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# SIG MANAGEMENT #\n",
    "##################\n",
    "def derive_private_key(row, curve=ecdsa.SECP256k1):\n",
    "    # Data\n",
    "    r = int(row.name[1], base=16)\n",
    "    s1, s2 = map(lambda s: int(s, base=16), row.s)\n",
    "    h1_str, h2_str = row.digests\n",
    "    h1, h2 = (\n",
    "        int(h1_str, base=16),\n",
    "        int(h2_str, base=16),\n",
    "    )\n",
    "\n",
    "    # Typecasting & constants\n",
    "    order = curve.order\n",
    "    generator = curve.generator\n",
    "\n",
    "    for s1, s2 in [\n",
    "        [s1, s2],\n",
    "        [order - s1, s2],\n",
    "        [s1, order - s2],\n",
    "        [order - s1, order - s2],\n",
    "    ]:\n",
    "        # Nonce derivation\n",
    "        s_diff_inv = pow((s1 - s2), -1, order)\n",
    "        nonce = ((h1 - h2) * s_diff_inv) % order\n",
    "        if r == (nonce * generator).x():\n",
    "            priv_key = pow(r, -1, order) * (s2 * nonce - h2) % order\n",
    "            sk = ecdsa.SigningKey.from_secret_exponent(\n",
    "                secexp=priv_key, curve=curve, hashfunc=None\n",
    "            )\n",
    "            vk = sk.get_verifying_key()\n",
    "            try:\n",
    "                # Check the validity of the private key, as we might have recovered -k if the user has published -s1 and -s2 (which are still valid) over the network.\n",
    "                vk.verify_digest(\n",
    "                    bytes.fromhex(f\"{r:064x}{s1:064x}\"),\n",
    "                    bytes.fromhex(h1_str),\n",
    "                )\n",
    "                vk.verify_digest(\n",
    "                    bytes.fromhex(f\"{r:064x}{s2:064x}\"),\n",
    "                    bytes.fromhex(h2_str),\n",
    "                )\n",
    "                return {\"nonce\": nonce, \"private_key\": priv_key}\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "\n",
    "    raise ValueError(\n",
    "        \"The nonce and the private key could not be recovered: the input signatures are probably invalid.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def derive_private_key_from_known_nonce(row, curve=ecdsa.SECP256k1):\n",
    "    order = curve.order\n",
    "\n",
    "    # Data\n",
    "    pubkey = row[\"pubkey\"]\n",
    "    r = int(row[\"r\"], base=16)\n",
    "    s = int(row[\"s\"], 16)\n",
    "    h_str = row[\"message digest\"]\n",
    "    h = int(h_str, 16)\n",
    "    nonce = row[\"nonce\"]\n",
    "\n",
    "    expected_vk = ecdsa.VerifyingKey.from_string(bytes.fromhex(pubkey), curve=curve)\n",
    "    # We have to be careful that for a given nonce and given signature, 2 private keys are valid, see ecdsa_tutorial (mirror_key)\n",
    "    priv_key = pow(r, -1, order) * (s * nonce - h) % order\n",
    "    sk = ecdsa.SigningKey.from_secret_exponent(\n",
    "        secexp=priv_key, curve=curve, hashfunc=None\n",
    "    )\n",
    "    vk = sk.get_verifying_key()\n",
    "    vk.verify_digest(\n",
    "        bytes.fromhex(f\"{r:064x}{s:064x}\"),\n",
    "        bytes.fromhex(h_str),\n",
    "    )\n",
    "\n",
    "    if vk != expected_vk:\n",
    "        nonce = (order - nonce) % order\n",
    "        # Let's fetch the alternative private key, that could have signed that message\n",
    "        priv_key = (pow(r, -1, order) * s * (2 * nonce) + priv_key) % order\n",
    "        sk = ecdsa.SigningKey.from_secret_exponent(\n",
    "            secexp=priv_key, curve=curve, hashfunc=None\n",
    "        )\n",
    "        vk = sk.get_verifying_key()\n",
    "        vk.verify_digest(\n",
    "            bytes.fromhex(f\"{r:064x}{s:064x}\"),\n",
    "            bytes.fromhex(h_str),\n",
    "        )\n",
    "\n",
    "        assert vk == expected_vk\n",
    "\n",
    "    return priv_key\n",
    "\n",
    "\n",
    "def derive_nonce_from_known_private_key(\n",
    "    h: int, r: int, d: int, s: int, curve=ecdsa.SECP256k1\n",
    "):\n",
    "\n",
    "    nonce = (pow(s, -1, curve.order) * (h + r * d)) % curve.order\n",
    "\n",
    "    assert (nonce * curve.generator).x() == r\n",
    "\n",
    "    return nonce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# POST CHECKING #\n",
    "#################\n",
    "def verify_private_keys(pubkey, privkey, curve=ecdsa.SECP256k1):\n",
    "    # Verify that every recovered private key is valid\n",
    "    vk_expected = ecdsa.VerifyingKey.from_string(bytes.fromhex(pubkey), curve=curve)\n",
    "    sk = ecdsa.SigningKey.from_secret_exponent(secexp=privkey, curve=curve)\n",
    "    vk = sk.get_verifying_key()\n",
    "\n",
    "    assert vk == vk_expected\n",
    "\n",
    "\n",
    "def verify_nonce(r: str, nonce: int, curve=ecdsa.SECP256k1):\n",
    "    r = int(r, 16)\n",
    "    assert (curve.generator * nonce).x() == r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# ADDRESSES #\n",
    "#############\n",
    "\n",
    "from utxo_utils.encoding.address import (\n",
    "    hash_to_32address,\n",
    "    hash_to_58address,\n",
    ")\n",
    "from utxo_utils.crypto.hash import hash160\n",
    "from bitcoincash.cashaddr import PUBKEY_TYPE, encode\n",
    "\n",
    "\n",
    "def generate_utxo_addresses(pubkey):\n",
    "\n",
    "    vk = ecdsa.VerifyingKey.from_string(\n",
    "        bytes.fromhex(pubkey),\n",
    "        curve=ecdsa.SECP256k1,\n",
    "    )\n",
    "    chains = [\"btc\", \"bch\", \"doge\", \"dash\", \"ltc\"]\n",
    "    pubkey_formats = [\"uncompressed\", \"compressed\", \"hybrid\"]\n",
    "    addresses = {\n",
    "        chain: {pubkey_format: []}\n",
    "        for pubkey_format in pubkey_formats\n",
    "        for chain in chains\n",
    "    }\n",
    "\n",
    "    for pubkey_format in pubkey_formats:\n",
    "        pubkey = vk.to_string(encoding=pubkey_format).hex()\n",
    "        h = hash160(pubkey)\n",
    "        script_hash = hash160(\"0014\" + h)\n",
    "        addresses[\"btc\"][pubkey_format] = [\n",
    "            hash_to_32address(h),\n",
    "            hash_to_58address(h, address_version=\"00\"),\n",
    "            hash_to_58address(  # Nested p2wpkh insided p2sh\n",
    "                script_hash,\n",
    "                \"05\",\n",
    "            ),\n",
    "        ]\n",
    "        addresses[\"bch\"][pubkey_format] = [\n",
    "            encode(\n",
    "                \"bitcoincash\",\n",
    "                PUBKEY_TYPE,\n",
    "                bytes.fromhex(h),\n",
    "            ),\n",
    "            hash_to_58address(h, address_version=\"00\"),\n",
    "        ]\n",
    "        addresses[\"doge\"][pubkey_format] = [hash_to_58address(h, address_version=\"1e\")]\n",
    "        addresses[\"dash\"][pubkey_format] = [hash_to_58address(h, address_version=\"4c\")]\n",
    "        # TODO add segwit\n",
    "        addresses[\"ltc\"][pubkey_format] = [\n",
    "            hash_to_58address(h, address_version=\"30\"),\n",
    "            hash_to_32address(h, hrp=\"ltc\"),\n",
    "            hash_to_58address(  # Nested p2wpkh insided p2sh\n",
    "                script_hash,\n",
    "                \"05\",\n",
    "            ),\n",
    "            hash_to_58address(  # Nested p2wpkh insided p2sh / new prefix for p2sh\n",
    "                script_hash,\n",
    "                \"32\",\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    return addresses\n",
    "\n",
    "\n",
    "def generate_flatten_utxo_addresses(pubkey):\n",
    "    addresses_dict = generate_utxo_addresses(pubkey)\n",
    "\n",
    "    chains, formats, addresses = [], [], []\n",
    "    for chain in addresses_dict:\n",
    "        for format in addresses_dict[chain]:\n",
    "            for address in addresses_dict[chain][format]:\n",
    "                chains.append(chain)\n",
    "                formats.append(format)\n",
    "                addresses.append(address)\n",
    "    return {\"chain_address\": chains, \"pubkey_format\": formats, \"address\": addresses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "bch_signatures_folders = [\n",
    "    \"/Users/vincent/Documents/PhD/Blockchains/UTXO/ecdsa-signatures/data/signatures/bch\",\n",
    "]\n",
    "\n",
    "signatures_folders = [\n",
    "    \"/Users/vincent/Documents/PhD/Blockchains/UTXO/ecdsa-signatures/data/signatures/btc\",\n",
    "    \"/Users/vincent/Documents/PhD/Blockchains/UTXO/ecdsa-signatures/data/signatures/dash\",\n",
    "    \"/Users/vincent/Documents/PhD/Blockchains/UTXO/ecdsa-signatures/data/signatures/bch\",\n",
    "    \"/Users/vincent/Documents/PhD/Blockchains/UTXO/ecdsa-signatures/data/signatures/ltc\",\n",
    "    \"/Users/vincent/Documents/PhD/Blockchains/UTXO/ecdsa-signatures/data/signatures/doge\",\n",
    "]\n",
    "\n",
    "\n",
    "def drop_cracked_keys(sig_df, cracked_keys_df):\n",
    "    return (\n",
    "        pd.merge(\n",
    "            sig_df,\n",
    "            cracked_keys_df[[\"pubkey\"]],\n",
    "            indicator=True,\n",
    "            how=\"outer\",\n",
    "            on=\"pubkey\",\n",
    "        )\n",
    "        .query('_merge==\"left_only\"')\n",
    "        .drop(\"_merge\", axis=1)\n",
    "    )\n",
    "\n",
    "\n",
    "# Read the data\n",
    "bch_sig_df = read_raw_data(bch_signatures_folders, check_signatures=True)\n",
    "other_sig_df = read_raw_data(signatures_folders, check_signatures=False)\n",
    "sig_df = pd.concat([bch_sig_df, other_sig_df])\n",
    "\n",
    "# ROUND 0: Retrieve the private keys from the use of repeated nonces\n",
    "repeated_nonces_df = prepare_data(sig_df)\n",
    "repeated_nonces_df[[\"nonce\", \"private_key\"]] = repeated_nonces_df.apply(\n",
    "    lambda row: derive_private_key(row, ecdsa.SECP256k1), axis=1, result_type=\"expand\"\n",
    ")\n",
    "# Check the validity of the results\n",
    "repeated_nonces_df.apply(\n",
    "    lambda row: verify_private_keys(row.name[0], row.private_key), axis=1\n",
    ")\n",
    "repeated_nonces_df.apply(lambda row: verify_nonce(row.name[1], row.nonce), axis=1)\n",
    "\n",
    "# List all 'r' values which are known\n",
    "known_nonces_df = (\n",
    "    repeated_nonces_df.reset_index()[[\"r\", \"block_timestamps\", \"nonce\", \"chain\"]]\n",
    "    .sort_values(  # We want the earliest time at which the 'r' value became vulnerable\n",
    "        by=[\"block_timestamps\"], key=lambda time_frame: time_frame.map(lambda x: x[1])\n",
    "    )\n",
    "    .groupby(by=\"r\", sort=False)\n",
    "    .head(1)\n",
    ")\n",
    "known_nonces_df[\"vulnerable_timestamp\"] = known_nonces_df[\"block_timestamps\"].apply(\n",
    "    lambda tf: tf[1]\n",
    ")\n",
    "known_nonces_df = known_nonces_df.rename(columns={\"chain\": \"r_chain_origin\"})\n",
    "known_nonces_df = known_nonces_df.drop(columns=[\"block_timestamps\"])\n",
    "known_nonces_df = known_nonces_df.set_index(keys=\"r\")\n",
    "\n",
    "# Format the results obtained so far\n",
    "prev_cnt = 0\n",
    "cracked_keys_df = pd.merge(\n",
    "    repeated_nonces_df.drop(\n",
    "        columns=[\"block_timestamps\", \"digests\", \"s\", \"nonce\"]\n",
    "    ).reset_index(),\n",
    "    known_nonces_df[[\"r_chain_origin\", \"vulnerable_timestamp\"]],\n",
    "    on=\"r\",\n",
    ").drop_duplicates()\n",
    "\n",
    "# Update the set of keys that remains to crack\n",
    "uncracked_keys_df = drop_cracked_keys(sig_df, cracked_keys_df).drop(\n",
    "    columns=[\"transaction_hash\", \"input_index\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19115\n",
      "2645\n",
      "21760\n",
      "605\n",
      "18958\n",
      "2802\n",
      "21760\n",
      "948\n",
      "18704\n",
      "3056\n",
      "21760\n",
      "1425\n",
      "18607\n",
      "3153\n",
      "21760\n",
      "1478\n",
      "18588\n",
      "3172\n",
      "21760\n",
      "1488\n",
      "18581\n",
      "3179\n",
      "21760\n",
      "1492\n",
      "18578\n",
      "3182\n",
      "21760\n",
      "1493\n"
     ]
    }
   ],
   "source": [
    "while cracked_keys_df[\"private_key\"].nunique() != prev_cnt:\n",
    "    prev_cnt = cracked_keys_df[\"private_key\"].nunique()\n",
    "    crackable_keys = pd.merge(uncracked_keys_df, known_nonces_df, on=\"r\")\n",
    "    if len(crackable_keys) == 0:\n",
    "        break\n",
    "    crackable_keys[\"private_key\"] = crackable_keys.apply(\n",
    "        derive_private_key_from_known_nonce, axis=1\n",
    "    )\n",
    "    crackable_keys = crackable_keys.drop(\n",
    "        columns=[\"s\", \"message digest\", \"nonce\", \"block_timestamp\"]\n",
    "    )\n",
    "    crackable_keys.apply(\n",
    "        lambda row: verify_private_keys(row.pubkey, row.private_key), axis=1\n",
    "    )\n",
    "\n",
    "    # Expand our set of known nonces\n",
    "    recoverable_nonces = pd.merge(\n",
    "        uncracked_keys_df,\n",
    "        crackable_keys[\n",
    "            [\"pubkey\", \"private_key\", \"vulnerable_timestamp\", \"r_chain_origin\"]\n",
    "        ],\n",
    "        how=\"inner\",\n",
    "        on=\"pubkey\",\n",
    "    )\n",
    "    recoverable_nonces[\"nonce\"] = recoverable_nonces.apply(\n",
    "        lambda row: derive_nonce_from_known_private_key(\n",
    "            int(row[\"message digest\"], 16),\n",
    "            int(row[\"r\"], 16),\n",
    "            row[\"private_key\"],\n",
    "            int(row[\"s\"], 16),\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    recoverable_nonces = recoverable_nonces[\n",
    "        [\"r\", \"nonce\", \"r_chain_origin\", \"vulnerable_timestamp\"]\n",
    "    ].set_index(\"r\")\n",
    "    known_nonces_df = (\n",
    "        pd.concat([recoverable_nonces, known_nonces_df])\n",
    "        .sort_values(by=\"vulnerable_timestamp\")\n",
    "        .groupby(by=\"r\", sort=False)\n",
    "        .head(1)\n",
    "    )\n",
    "\n",
    "    # Expand our set of cracked keys\n",
    "    cracked_keys_df = pd.concat([crackable_keys, cracked_keys_df]).drop_duplicates()\n",
    "\n",
    "    # Update the set of keys that remains to crack\n",
    "    uncracked_keys_df = drop_cracked_keys(sig_df, cracked_keys_df).drop(\n",
    "        columns=[\"transaction_hash\", \"input_index\"]\n",
    "    )\n",
    "\n",
    "    # Print some stats\n",
    "    print(uncracked_keys_df[\"pubkey\"].nunique())\n",
    "    print(cracked_keys_df[\"pubkey\"].nunique())\n",
    "    print(sig_df[\"pubkey\"].nunique())\n",
    "    print(len(known_nonces_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "uncracked_keys_df[\"pubkey_chain\"] = (\n",
    "    uncracked_keys_df[\"chain\"] + \":\" + uncracked_keys_df[\"pubkey\"]\n",
    ")\n",
    "\n",
    "uncracked_keys_df\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(uncracked_keys_df[[\"pubkey_chain\", \"r\"]].to_numpy().tolist())\n",
    "connected_components = list(nx.connected_components(G))\n",
    "connected_components = sorted(\n",
    "    connected_components,\n",
    "    key=lambda comp: len(list(filter(lambda s: \":\" in s, comp))),\n",
    "    reverse=True,\n",
    ")\n",
    "cycles = nx.cycle_basis(G)\n",
    "print(len(cycles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18575\n",
      "3185\n",
      "21760\n",
      "1496\n",
      "18573\n",
      "3187\n",
      "21760\n",
      "1498\n",
      "18572\n",
      "3188\n",
      "21760\n",
      "1500\n",
      "18570\n",
      "3190\n",
      "21760\n",
      "1502\n",
      "18569\n",
      "3191\n",
      "21760\n",
      "1504\n",
      "18568\n",
      "3192\n",
      "21760\n",
      "1506\n",
      "18567\n",
      "3193\n",
      "21760\n",
      "1508\n",
      "18566\n",
      "3194\n",
      "21760\n",
      "1510\n",
      "18565\n",
      "3195\n",
      "21760\n",
      "1512\n",
      "18564\n",
      "3196\n",
      "21760\n",
      "1514\n",
      "18563\n",
      "3197\n",
      "21760\n",
      "1516\n",
      "18562\n",
      "3198\n",
      "21760\n",
      "1518\n",
      "18561\n",
      "3199\n",
      "21760\n",
      "1520\n",
      "18560\n",
      "3200\n",
      "21760\n",
      "1522\n",
      "18559\n",
      "3201\n",
      "21760\n",
      "1524\n",
      "18558\n",
      "3202\n",
      "21760\n",
      "1526\n",
      "18557\n",
      "3203\n",
      "21760\n",
      "1528\n",
      "18556\n",
      "3204\n",
      "21760\n",
      "1530\n",
      "18554\n",
      "3206\n",
      "21760\n",
      "1532\n",
      "18553\n",
      "3207\n",
      "21760\n",
      "1534\n",
      "18551\n",
      "3209\n",
      "21760\n",
      "1536\n",
      "18549\n",
      "3211\n",
      "21760\n",
      "1538\n",
      "18549\n",
      "3211\n",
      "21760\n",
      "1540\n",
      "18548\n",
      "3212\n",
      "21760\n",
      "1542\n",
      "18548\n",
      "3212\n",
      "21760\n",
      "1544\n",
      "18547\n",
      "3213\n",
      "21760\n",
      "1546\n",
      "18547\n",
      "3213\n",
      "21760\n",
      "1548\n",
      "18546\n",
      "3214\n",
      "21760\n",
      "1550\n",
      "18546\n",
      "3214\n",
      "21760\n",
      "1552\n",
      "18545\n",
      "3215\n",
      "21760\n",
      "1554\n",
      "18545\n",
      "3215\n",
      "21760\n",
      "1556\n",
      "18544\n",
      "3216\n",
      "21760\n",
      "1558\n",
      "18544\n",
      "3216\n",
      "21760\n",
      "1560\n",
      "18542\n",
      "3218\n",
      "21760\n",
      "1562\n",
      "18542\n",
      "3218\n",
      "21760\n",
      "1564\n",
      "18542\n",
      "3218\n",
      "21760\n",
      "1566\n",
      "18536\n",
      "3224\n",
      "21760\n",
      "1572\n",
      "18534\n",
      "3226\n",
      "21760\n",
      "1574\n",
      "18532\n",
      "3228\n",
      "21760\n",
      "1576\n",
      "18530\n",
      "3230\n",
      "21760\n",
      "1578\n",
      "18528\n",
      "3232\n",
      "21760\n",
      "1580\n",
      "18528\n",
      "3232\n",
      "21760\n",
      "1582\n",
      "18528\n",
      "3232\n",
      "21760\n",
      "1584\n",
      "18526\n",
      "3234\n",
      "21760\n",
      "1586\n",
      "18526\n",
      "3234\n",
      "21760\n",
      "1588\n",
      "18525\n",
      "3235\n",
      "21760\n",
      "1590\n",
      "18525\n",
      "3235\n",
      "21760\n",
      "1592\n",
      "18524\n",
      "3236\n",
      "21760\n",
      "1594\n",
      "18524\n",
      "3236\n",
      "21760\n",
      "1596\n",
      "18523\n",
      "3237\n",
      "21760\n",
      "1598\n",
      "18522\n",
      "3238\n",
      "21760\n",
      "1600\n",
      "18520\n",
      "3240\n",
      "21760\n",
      "1602\n",
      "18518\n",
      "3242\n",
      "21760\n",
      "1604\n",
      "18516\n",
      "3244\n",
      "21760\n",
      "1606\n",
      "18514\n",
      "3246\n",
      "21760\n",
      "1608\n",
      "18514\n",
      "3246\n",
      "21760\n",
      "1610\n",
      "18514\n",
      "3246\n",
      "21760\n",
      "1612\n",
      "18512\n",
      "3248\n",
      "21760\n",
      "1614\n",
      "18510\n",
      "3250\n",
      "21760\n",
      "1616\n",
      "18510\n",
      "3250\n",
      "21760\n",
      "1618\n",
      "18508\n",
      "3252\n",
      "21760\n",
      "1620\n",
      "18507\n",
      "3253\n",
      "21760\n",
      "1622\n",
      "18505\n",
      "3255\n",
      "21760\n",
      "1624\n",
      "18503\n",
      "3257\n",
      "21760\n",
      "1626\n",
      "18501\n",
      "3259\n",
      "21760\n",
      "1628\n",
      "18499\n",
      "3261\n",
      "21760\n",
      "1630\n",
      "18497\n",
      "3263\n",
      "21760\n",
      "1632\n",
      "18495\n",
      "3265\n",
      "21760\n",
      "1634\n",
      "18493\n",
      "3267\n",
      "21760\n",
      "1636\n",
      "18491\n",
      "3269\n",
      "21760\n",
      "1638\n",
      "18491\n",
      "3269\n",
      "21760\n",
      "1640\n"
     ]
    }
   ],
   "source": [
    "def solve(m, b, pos, pubkeys, r, curve=ecdsa.SECP256k1):\n",
    "    m = deepcopy(m)\n",
    "\n",
    "    for x in pos:\n",
    "        for j in range(len(b) // 2):\n",
    "            m[x][j] = -m[x][j] % ecdsa.SECP256k1.order if m[x][j] != 0 else 0\n",
    "\n",
    "    m = Matrix(m)\n",
    "    b = Matrix(b)\n",
    "    sol = m.inv_mod(ecdsa.SECP256k1.order) * b % ecdsa.SECP256k1.order\n",
    "\n",
    "    private_keys = {}\n",
    "    for i, s in enumerate(sol[len(b) // 2 :]):\n",
    "        sk = ecdsa.SigningKey.from_secret_exponent(\n",
    "            secexp=s,\n",
    "            curve=ecdsa.SECP256k1,\n",
    "        )\n",
    "        vk = sk.get_verifying_key()\n",
    "        pubkey = vk.to_string(\"compressed\").hex()\n",
    "        private_keys[pubkey] = s\n",
    "\n",
    "    if set(private_keys.keys()) == set(pubkeys):\n",
    "        nonces = {}\n",
    "        for nonce in sol[: len(b) // 2]:\n",
    "            r_int = (curve.generator * nonce).x()\n",
    "            r_int_to_str = {int(r_str, 16): r_str for r_str in r}\n",
    "            nonces[r_int_to_str[r_int]] = nonce\n",
    "\n",
    "        return nonces, private_keys\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def solve_for_all_alterations(m, b, pubkeys, r):\n",
    "    pos_s = [i for i in range(len(b))]\n",
    "    for L in range(len(pos_s) + 1):\n",
    "        for subset in itertools.combinations(pos_s, L):\n",
    "            nonces, priv_keys = solve(m, b, subset, pubkeys, r)\n",
    "            if nonces and priv_keys:\n",
    "                assert set(nonces.keys()) == set(r)\n",
    "                return nonces, priv_keys\n",
    "\n",
    "    raise ValueError(\"The private keys could not be recovered.\")\n",
    "\n",
    "\n",
    "def get_rows_from_cycle(df, cycle):\n",
    "    to_fetch = []\n",
    "\n",
    "    for node, next_node in zip(cycle, cycle[1:] + [cycle[0]]):\n",
    "        if \":\" in node:\n",
    "            pk = node.split(\":\")[1]\n",
    "            r = next_node\n",
    "        else:\n",
    "            pk = next_node.split(\":\")[1]\n",
    "            r = node\n",
    "        to_fetch.append((pk, r))\n",
    "\n",
    "    rows = df.loc[to_fetch].sort_index()\n",
    "    return rows\n",
    "\n",
    "\n",
    "def build_system_matrix(rows, curve=ecdsa.SECP256k1):\n",
    "    if len(rows) % 2:\n",
    "        raise ValueError(\"The number of signatures should be even.\")\n",
    "\n",
    "    pk_to_pos = {pk: i for i, pk in enumerate(set(map(lambda i: i[0], rows.index)))}\n",
    "    r_to_pos = {\n",
    "        r: i for i, r in enumerate(set(map(lambda i: int(i[1], 16), rows.index)))\n",
    "    }\n",
    "\n",
    "    m = [[0 for _ in range(len(cycle))] for _ in range(len(cycle))]\n",
    "    b = []\n",
    "    dim = len(rows)\n",
    "\n",
    "    for i, tuple in enumerate(rows.itertuples()):\n",
    "        pk, r, s, h = (\n",
    "            tuple.Index[0],\n",
    "            int(tuple.Index[1], 16),\n",
    "            int(tuple.s, 16),\n",
    "            int(tuple.h, 16),\n",
    "        )\n",
    "        m[i][r_to_pos[r]] = s\n",
    "        m[i][pk_to_pos[pk] + dim // 2] = -r % ecdsa.SECP256k1.order\n",
    "        b.append(h)\n",
    "\n",
    "    return m, b\n",
    "\n",
    "\n",
    "indexed_df = uncracked_keys_df.set_index(keys=[\"pubkey\", \"r\"]).rename(\n",
    "    columns={\"message digest\": \"h\"}\n",
    ")\n",
    "for cycle in cycles[:]:\n",
    "    rows = get_rows_from_cycle(indexed_df, cycle)\n",
    "    r_chain_origin = (\n",
    "        rows.iloc[0].chain if rows[\"chain\"].nunique() == 1 else rows[\"chain\"].unique()\n",
    "    )\n",
    "    vulnerable_timestamp = rows[\"block_timestamp\"].max()\n",
    "    m, b = build_system_matrix(rows)\n",
    "\n",
    "    seen = set()\n",
    "    pubkeys = [\n",
    "        tuple.Index[0]\n",
    "        for tuple in rows.itertuples()\n",
    "        if not (tuple.Index[0] in seen or seen.add(tuple.Index[0]))\n",
    "    ]\n",
    "    seen = set()\n",
    "    r = [\n",
    "        tuple.Index[1]\n",
    "        for tuple in rows.itertuples()\n",
    "        if not (tuple.Index[1] in seen or seen.add(tuple.Index[1]))\n",
    "    ]\n",
    "\n",
    "    nonces, private_keys = solve_for_all_alterations(m, b, pubkeys, r)\n",
    "\n",
    "    # Expand our set of nonces\n",
    "    nonces_rows = []\n",
    "    for r, nonce in nonces.items():\n",
    "        nonces_rows.append((r, nonce, r_chain_origin, vulnerable_timestamp))\n",
    "    known_nonces_df = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                nonces_rows, columns=known_nonces_df.reset_index().columns\n",
    "            ).set_index(keys=\"r\"),\n",
    "            known_nonces_df,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Expand our set of cracked keys\n",
    "    rows = (\n",
    "        rows.drop(columns=[\"s\", \"h\", \"pubkey_chain\"])\n",
    "        .rename(columns={\"block_timestamp\": \"vulnerable_timestamp\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    rows[\"r_chain_origin\"] = r_chain_origin\n",
    "    rows[\"private_key\"] = rows[\"pubkey\"].apply(lambda pubkey: private_keys[pubkey])\n",
    "    cracked_keys_df = pd.concat([rows, cracked_keys_df]).drop_duplicates()\n",
    "\n",
    "    # Update the set of keys that remains to crack\n",
    "    uncracked_keys_df = drop_cracked_keys(sig_df, cracked_keys_df).drop(\n",
    "        columns=[\"transaction_hash\", \"input_index\"]\n",
    "    )\n",
    "\n",
    "    # Print some stats\n",
    "    print(uncracked_keys_df[\"pubkey\"].nunique())\n",
    "    print(cracked_keys_df[\"pubkey\"].nunique())\n",
    "    print(sig_df[\"pubkey\"].nunique())\n",
    "    print(len(known_nonces_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18473\n",
      "3287\n",
      "21760\n",
      "1571\n",
      "18464\n",
      "3296\n",
      "21760\n",
      "1571\n"
     ]
    }
   ],
   "source": [
    "while cracked_keys_df[\"private_key\"].nunique() != prev_cnt:\n",
    "    prev_cnt = cracked_keys_df[\"private_key\"].nunique()\n",
    "    crackable_keys = pd.merge(uncracked_keys_df, known_nonces_df, on=\"r\")\n",
    "    if len(crackable_keys) == 0:\n",
    "        break\n",
    "    crackable_keys[\"private_key\"] = crackable_keys.apply(\n",
    "        derive_private_key_from_known_nonce, axis=1\n",
    "    )\n",
    "    crackable_keys = crackable_keys.drop(\n",
    "        columns=[\"s\", \"message digest\", \"nonce\", \"block_timestamp\"]\n",
    "    )\n",
    "    crackable_keys.apply(\n",
    "        lambda row: verify_private_keys(row.pubkey, row.private_key), axis=1\n",
    "    )\n",
    "\n",
    "    # Expand our set of known nonces\n",
    "    recoverable_nonces = pd.merge(\n",
    "        uncracked_keys_df,\n",
    "        crackable_keys[\n",
    "            [\"pubkey\", \"private_key\", \"vulnerable_timestamp\", \"r_chain_origin\"]\n",
    "        ],\n",
    "        how=\"inner\",\n",
    "        on=\"pubkey\",\n",
    "    )\n",
    "    recoverable_nonces[\"nonce\"] = recoverable_nonces.apply(\n",
    "        lambda row: derive_nonce_from_known_private_key(\n",
    "            int(row[\"message digest\"], 16),\n",
    "            int(row[\"r\"], 16),\n",
    "            row[\"private_key\"],\n",
    "            int(row[\"s\"], 16),\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    recoverable_nonces = recoverable_nonces[\n",
    "        [\"r\", \"nonce\", \"r_chain_origin\", \"vulnerable_timestamp\"]\n",
    "    ].set_index(\"r\")\n",
    "    known_nonces_df = (\n",
    "        pd.concat([recoverable_nonces, known_nonces_df])\n",
    "        .sort_values(by=\"vulnerable_timestamp\")\n",
    "        .groupby(by=\"r\", sort=False)\n",
    "        .head(1)\n",
    "    )\n",
    "\n",
    "    # Expand our set of cracked keys\n",
    "    cracked_keys_df = pd.concat([crackable_keys, cracked_keys_df]).drop_duplicates()\n",
    "\n",
    "    # Update the set of keys that remains to crack\n",
    "    uncracked_keys_df = drop_cracked_keys(sig_df, cracked_keys_df).drop(\n",
    "        columns=[\"transaction_hash\", \"input_index\"]\n",
    "    )\n",
    "\n",
    "    # Print some stats\n",
    "    print(uncracked_keys_df[\"pubkey\"].nunique())\n",
    "    print(cracked_keys_df[\"pubkey\"].nunique())\n",
    "    print(sig_df[\"pubkey\"].nunique())\n",
    "    print(len(known_nonces_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00926cd31b298377704537d0a88af069f15d014e6e9b9dd25cb6c0e950bfeedd00', '333ba2b0524803edb7136d5bb583a9e4af9f98cb3c626a25d97cd1bbf0321ccd']\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "uncracked_keys_df[\"pubkey_chain\"] = (\n",
    "    uncracked_keys_df[\"chain\"] + \":\" + uncracked_keys_df[\"pubkey\"]\n",
    ")\n",
    "\n",
    "uncracked_keys_df\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(uncracked_keys_df[[\"pubkey_chain\", \"r\"]].to_numpy().tolist())\n",
    "connected_components = list(nx.connected_components(G))\n",
    "connected_components = sorted(\n",
    "    connected_components,\n",
    "    key=lambda comp: len(list(filter(lambda s: \":\" in s, comp))),\n",
    "    reverse=True,\n",
    ")\n",
    "print(list(filter(lambda s: \":\" not in s, connected_components[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_private_keys_with_addresses = cracked_keys_df\n",
    "all_private_keys_with_addresses[[\"chain_address\", \"pubkey_format\", \"address\"]] = (\n",
    "    cracked_keys_df.apply(\n",
    "        lambda row: generate_flatten_utxo_addresses(row[\"pubkey\"]),\n",
    "        axis=1,\n",
    "        result_type=\"expand\",\n",
    "    )\n",
    ")\n",
    "all_private_keys_with_addresses = all_private_keys_with_addresses.explode(\n",
    "    [\"chain_address\", \"pubkey_format\", \"address\"]\n",
    ")\n",
    "all_private_keys_with_addresses[\n",
    "    [\"address\", \"chain_address\", \"pubkey\", \"pubkey_format\"]\n",
    "].to_parquet(\"addresses.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "addresses = pd.read_parquet(\"../data/addresses.parquet\")\n",
    "len(addresses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
